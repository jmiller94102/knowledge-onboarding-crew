The thing that a lot of people are missing right now, the fact that like it's just hard to use Claude Code and figure out where you need to go.
You have this magical eight ball that can literally summon you a pizza to generate your next billion dollar idea.
And now that you have this tool, how do you actually use it productively? 'cause when I use it, it doesn't always work.
And then other people online say it doesn't work. And then when I use it this other way, all of a sudden, like I'm just in the zone burning tokens at.
A million miles an hour. And so how can we actually leverage this intelligence And this conversation today is gonna actually help us figure that out. There is no reason that tools can't be constructed at this moment with the technology as it currently is that will reliably let you give it a task, and you come back in an hour and the whole thing has been validated.
Claude code has quickly taken over as the top choice for AI coding tools, but most people are only using a fraction of its capabilities.
This tool is way more powerful than people realize, and even I wasn't aware of some of the incredible functionality that we discussed today.
So in episode 52 of tool Use brought to you by Tool Hive, we have two cloud code experts joining us to tell us how the pros are using it.
There are so many great tips in this video. Every cloud code user will learn something new. We cover how to optimally set up cloud code, the benefits of different thinking modes, context management techniques, sub-agents, incredible hooks and slash commands, and a lot more.
We're joined by Ray Fernando, a former Apple engineer for 12 years, and my favorite AI streamer teaching the whole world how to use ai.
Recapio creative
AI Youtube Summarizer
Summarize or Chat with AI, read insights in seconds, save hours. Translate to any language

Try for Free
Sponsored

He's always cooking up some great content. And Eric Bus, an avid builder who's deeply curious and an Anthropic super fan, he has tweeted out more high value insights on Cloud code than anyone else I've seen. If you're not following these two guys, you're definitely falling behind.
This is a cloud code masterclass. when you install Claude code on a, on a new machine, what, what's the first thing you do? How do you get started? First, I set up an alias for the letter C so that whenever I type C. It opens Claude with dangerously skip permissions flag. Uh, so I can very quickly hop around to a new project folder, uh, in the terminal and I can just type C and then it's open and I can start asking questions. I also try to do keyboard replacements, so like, you know, your keyboard text replacement. So I replaced the word, the letter U with ultra think.
And I replaced the, the letter W with what happened ultra think and make a plan before coding.
So I have a set of just principles that speed up the workflow quite a bit. The next thing I do is install cloud code docs, Uh, and so this, this documentation allows. A simple way for cloud code to understand what its capabilities are, um, is as a base set of internal. I, I guess it's, it's not specifically training, but Anthropic provides it with a set of instructions about what it can and can't do. But in order to get more information about the, the depth of its capabilities, it has to do a web fetch. So if you ask it, can you do this? How do I use. Slash commands, how do I use agents, things like that.
Then it has to go to anthropic docs, and even though if you go to the documentation website, you can click on, uh, copy markdown, it'll give you the markdown file for some reason. Right now, cloud code doesn't go straight to that file. It goes to the webpage.
It does a, a command which converts it to Markdown and it downloads all this stuff, and it takes a little bit of extra processing to do that.
But it doesn't seem to have a direct manifest to know where all the other documentations are. So the Cloud code docs, what they do is it's a one line installer.
And you install it, and then it puts the documentations locally and it's kept up to date. There's a GitHub action that syncs with the Anthropic Claude code docs every three hours. It downloads that the, so when you ask a question, there's a hook, and what the hook does is it, it tells, uh, so you, you do forward slash docs space and then you ask your question.
That's the slash command. The slash command tells Claude code where to look for the docs instead of going to the Anthropic's websites.
Go to your local path where you've already got the docs installed that are up to date. It also, uh, has a hook.
And the hook does it get fetch? And if it sees new content from the. The, uh, repository on GitHub, then it will do a GI pull and it does that good pull and finishes it before it sends the request to whatever you ask the docs for.
So whatever information you want to know. So basically it allows Claude to be able to answer any intricate to deep questions you have about the new things that came out. So you can say what's new, it'll tell you, here's a diff from the last.
Uh, a time that documents were updated for these docs, they were updated and these weren't, and then based on those updates, I can see here's a summary of all the things it was this many days ago. And you can click on the link to the GitHub and you can see the diff and you can click on the link to the source and you can see Anthropic's Claude Code Doc for that documentation. And then you just talk to and say, how does, how does a slash commands work?
How can I use it with hooks? What, what are the sub agents? Uh, how can I take advantage of this tool in my project that I'm working on right now that would, and, and what would the trade offs be, for example? Or what are some un uh, like unexpected ways that I can connect these together that I might not be thinking of, uh, based on this new tool that just came out, this new feature. And so having the docs locally is, I think, the biggest, um, high yield thing that you can do whenever you first get started with cloud code. it gives you the tool to learn the tool.
Um, but I'd like some clarity on a couple things you mentioned. So you said you set the flag dangerously skip permissions, which is probably the most intense sounding flag of any CLI I've come across. Is that YOLO mode or what does that enable into your workflow?
Yeah, so there's a lot of prompting you can do and there are trade offs with this flag. That's what I do. And obviously for, it's not prescriptive for everyone else, it depends on your use case and, but if you're like trying to get used to what Claude does, uh, then. It's, you know, if you don't turn that flag on, whenever a cloud code first came out, I wrote a wrapper that would, uh, instead of calling Claude directly, it would call this other tool, which would then call cloud code and then attach to the terminal session and send commands to it, recursively. And so it basically convinced it, it was in a sandbox, in a, uh, running a Docker container that gave, got around, kind of did the same thing. And then they give us a flag for dangers, gift permissions.
And what that does is it makes it where. A lot of the things where you ask it to do something, has a right to a file, maybe it has access to read a file, but maybe it can't change files in this path or, or run this tool or whatever, then it prompts you, are you sure you wanna do this?
Do you wanna do it for this session? Or, or if you have a subagent and you wanna give it access to do stuff, maybe you have to pass in exactly the tools you want to have that.
And so it's a little bit more complexity, uh, on you upfront and that's fine. But it essentially is YOLO mode.
So if you're comfortable in your machine and your environment and the tasks that's in front of you to do yellow mode, that's great.
Now I set up a VM that's separate to all this, um, so that I could do snapshots and give it access to everything. And it's running on top of my Mac and a Linux. So it's a, it is a separate sandbox environment basically.
So I feel comfortable with my yolo, but uh, in most cases I think YOLO is, I haven't had any issues with it, doing anything, um, in folders.
I didn't want it to. There's lots of ways to prevent that. Even if you, you can use a hook, for example, to say, before you run this command, uh, make sure it's not in these areas. And the hook is a separate process that runs outside of Claude code.
And so it can, uh, prevent things from happening as an example. and Ray. Uh, Eric mentioned thinking mode, and I know there's a few different types of commands you can throw in there. How, how does thinking mode affect the way you use Claude code? Yeah, so thinking mode is a mode that I would use a lot for research or specific types of tasks. Uh, keep in mind there's like four different modes. So like there's the ultra think, which is can use about 32,000 tokens for thinking.
And then it goes all the way down pretty much by half. So there's like think harder, which would be 16,000.
Uh, I think, like, I forget like, actually let me pull this up right now 'cause I have it on my blog. And I'm ultra think all the way, just, just take it all, I've, I've learned an interesting lesson and this is kind of why I, I love that we're doing the podcast because I used to be just all Uber Ultra think all the time.
And I, I, I noticed that I get variable results and I kept kind of digging into why, and I kind of went back to like something that I learned, which is basically less, is more type of thing. So I have an article here that tells me, uh, from my website, Ray fernando.ai, and here basically you have the four thinking modes.
So you have think, think hard, think harder, and ultra think as no spaces.
And these are key terms that Claude Code is gonna be looking for inside of.
Its environment and this one is gonna consume about 32,000 tokens. You're gonna be 16 here.
This is gonna be about eight and about 4,000 for for thinking. And it's also important to note that like these thinking windows are, are very important for helping users understand like how much context you're gonna be using up.
And if you're using ultra think all the time, what I discovered is that. It's really good to just have your conversation isolated to a specific concern or problem that you're looking for, and you kind of wanna maybe eventually start breaking that up and I could show you why, because of the, the, the thing that everyone's talking about these days called like, you know, context engineering or context window. So I built this simulation to kind of help us understand a little bit of kind of what you're doing if you do like an ultra type think type of thing. So if you have some type of input about like, you know, 2000 tokens that you're saying, here's some relevant information that I'm trying to do. And then we do agent thinking, you pop in 8, 16, 24, you know, uh, a bunch of thinking tokens. So you, you use all this thinking token window to, to have it do some stuff.
What cloud code's gonna do is gonna do some tool calling to grab some stuff, maybe some code files and various stuff.
And then eventually it's gonna try to generate like a little plan or some output or something like that. So maybe it generates 32,000 tokens.
Sometimes I've seen it, you know, generate like go through 40,000 or 50,000 tokens, just searching through your code base and grabbing different files and things.
Uh, so once this context window has filled up. Then you're gonna say, okay, cool, yeah, let's go ahead and build my next billion dollar SaaS.
And then you're like, I want you to do these types of things and make sure you grab all these other files as well. And then if you still have ultra think turned on, it's gonna try to like then digest those types of things there. It'll do some more tool calling in your code, and then it's gonna start to do some more code output, and then eventually the output will not be kind of what you want it to be. And that's just two different conversations that you've had and you're like, dang, I have a 200 K token context window.
And like my code isn't as good as what everyone's saying. And that's usually quite the problem here and there.
There actually is a study that's been done by Chroma DB that talks about context rot in, in a funny way, that the effective token context window starts to really fall off the cliff after about 50% for most use cases.
And it gets distracted if you throw too many different problems.
And so it's really important to kind of start with a conversation. And then maybe just kind of start branching it off into different components that you want to solve for something like that. So you as a human will want to review this giant research output that you just got out, and then see if you can kind of break that out into a different plan by doing slash clear.
And another technique people use is maybe spawning subagents. So if we kind of reset this here, let's just say we have a simple query that we do, and this is just kind of what happens. You know, the agent comes back with some more code output or something like that, or did some work, and now you're saying, okay, I have this plan.
Now let's go ahead and see if we can execute on this in these different chunks. And you can even ask Claude Co to generate some subagents to try to figure out.
What it can do from there. So what that does now is it spawns off a whole new session with another set of 200,000 token context window, and that way you can just go ahead and follow on requests and so forth. And each of those subagent, you kind of want them to really isolate and focus on one specific problem so that way it doesn't affect the main agent that's at the very top.
If there's any reporting or things that need to go back, it will send it back. But then you're now kind of dealing with something that's a little bit more intelligent kind of thinking of yourself as like a manager. And you're handing off these, you know, really intense tasks to people to really focus on and get their work done. And that leads to very high code quality and high code generation.
So back to kind of like the previous example where we talked about like, think, think hard, think harder, and ultra think you may want to think about maybe if I try something like think hard, maybe not, not as much tokens, but it can still get me some pretty good results because I still value the accuracy.
Maybe for the first request, I want to ultra think because I want to go through a lot more thinking and, and then, 'cause I've had this problem for a while and I've tried all these techniques and none of these work. So I'm gonna feed that in as part of the plan saying, I've tried these techniques, these are some code snippets we've tried, and here's some documentation I want you to now ultra think and just, you know, the bazooka comes out and just, you know, go ahead and solve that problem. And that's just a, a little bit of context in, in terms of like how.
This stuff is kind of managed, but I feel like this can provide some oversight on maybe how do you choose think versus think harder or ultra think, and then why some of this visualization for you to see is important, which kinda goes into the topic of subagents. And I'd love, you know, Eric, to talk a little bit more maybe on how do you figure out these like subagents and maybe, you know, how, how do you kind of best take advantage with claw code in these types of environments?
Yeah. That was awesome. I loved your visualization. I think that's a great explanation. I feel like these are two, um, related, but independent problems that feed into each other. The issue of, um, you know, it's like Miller's Law, I think it is, with humans. You can hold five to seven chunks of information and working memory at one time, and then, you know, you get overwhelmed with stuff. So if you, it doesn't matter how good your model is at instruction following benchmarks at some point, if you give. But too many, um, do this, don't do that.
Critical. You must do this. Like, it's very confusing, especially the whole lot of, uh, the more you fill the information space, it can't attend to everything equally at all times.
And every, you can think of it, like every percentage you focus it's attention on one thing. It's, it's got a little bit less for something else.
Usually that's separate. And if you keep the conversation going over time as it gets close to its mass capacity.
Its quality of output is definitely gonna degrade. It's been like that since the beginning, and I feel like for a long time, models are probably gonna do that, and that's why Claude Code has this auto compact feature, which is really cool, but you don't want to get to like 90%.
Like when it says there's 10% remaining, it's probably time to start over anyway. And so the, the issue of like, the reason I I spammed the ultra think thing is, is I do, I tend to like start a fresh session. I clear and I have an ultra think based off a previous plan that was well documented because I feel like when you do a project, it's really important to plan, not just, you know, what your Claude.md should be.
But like what the file structure should be, what the instructions that the model should have that kind of deviate from its baseline that you want to change a little bit. So like, Hey, don't randomly create a bunch of files. Let's just stick with these documentation files. Don't change that many.
You can add, you can change the content, but don't add new ones. Uh, the maximum amount of this many lines per file, et cetera.
And then keep them up to date. You can do that with something like a hook, or you can do that with the slash command, which is what I tend to use for context reasons.
But then at the end, uh, you know, you clear. Or you compact or whatever before it's too large.
And so you can still manage your context, uh, without and have high quality responses.
And, and in that case, ultra think could be advantageous. I found it to work really great for me, but at the same time, uh, it does take up a lot more tokens and you just have to watch that more carefully. But if you're on that, like I'm on the max plan, the 20 x, it gives you, um, you know, a lot of tokens to play with in a five hour session. And so, um, in that case, if I start running close to it, I have a session tracker tool that will tell me how many to approximately how many hours or minutes I have left in my five hours so that I can know it's gonna trigger another one.
And it gives me a countdown. I can see it in the menu bar. And so like, okay, I'm gonna hold off and I'm gonna wait until like the next day so I don't burn through too many in a month. But then the clause's gonna anthropics changing that at some point. We don't know the details yet. I think I'm curious, Eric, about your hook workflow. Like how do you. What are some of the hooks that you've tried that?
'cause I feel like everyone's tried different hooks and none of them have really stuck around for me too much.
You know, like, you know, I, I from generating sounds to having it do extreme code reviews where nothing happens until like, you know, it iterates on itself and I'm like, yeah, that's an infinite loop right now. It's like, I can't get outta my agent.
So Have you found any hooks that you've found that are pretty useful for yourself that you've either generated for your own tool use or just kinda curious as of what you've been cooking with? one thing that I have is a. I mentioned the Claude Docs.
So that's a hook because it does a fetch every time you do a read request from the path where the docs are installed, then it detects that you're trying to read in that path, and then it triggers a hook and the hook calls a script, and the script does a thing and the thing gets done in that response, comes back and gets fed into the Claude model or, or like updates something before. The request from the user goes to the model to be processed if you do a preto use hook. Um, so as far as. Other things I do.
So yes, I have something called a hook that I love and I, I wouldn't do any, so many of my projects rely on this one hook and, uh, it has definitely stuck around.
I think it'll be there forever, for a very long time anyway. And it is the indexer. So I have a project that is, uh, cloud code.
It is called project Index. And so a long time ago, like when, but when chat BGPT first came out and I like got obsessed with, okay, this is gonna write my code for me. And literally since then I haven't written more than a, a couple hundred lines of code. It's been entirely vibe coding from day one, well, probably from like day three accurately.
But, but I, I, like a monkey was sitting there, I felt like, uh, figuring out, okay, here's the code and here's the my code in my project, and then I've got a. Figure out how to merge the two together by hand. So I wrote this tool that like you just copy the clipboard and it automatically, uh, copies all the code because it does unified gi diff format and it knows the paths where it's supposed to be merged and then just merges all the code in looks at the clipboard. Um. But I realized early on to get the best results, what you wanna do is have as high signal and as little noise as possible to the models. So we don't overwhelm their context with stuff that isn't relevant to the task at hand. So whatever you want them to do, you need to give them as much information that they need to know. What are the documentation updates since in the API, since what their training was and what are the files and not the other files they don't need, just the files that they do need.
So is there a pre-processing step that could be useful to the models to be able to optimize their response?
Uh, their quality of response. And so I determined then the best solution was step one.
Uh, take a minified version of the entire code base. And so for every file, I don't mean like web ification, where it's obfuscated and, and random, like the letters instead of variable names up. I mean like, uh, you know, like a UML style abstraction, but you have the actual import statements, methods, signatures, maybe root level constants or something like that, return types, et cetera.
And all the dependencies for every file on your project that's not in dot get ignored. So this is like a project.
It's got the path, like the project tree structure, and it has for all of these files, a little bit of information about where they sit and what they relate to and what content they contain. And so I have a hook that goes through the project every time a file is changed and it does this and it updates the index. So there's a, there's a project index, all caps json that sits in the root of every project. And so this hook maintains that. And it's outside of Claude doesn't know about it because it's not inside it, it's lifecycle. The hooks sit outside of the cloud you're talking to. So it doesn't, it doesn't, uh, dilute the, the context window of Claude to use a hook. So this hook updates the project index file, and then whenever I ask for a change, I can, for example, spin up a subagent. I can say, Hey, use this subagent to look at this, uh, your project index, and figure out just which files, which lines are needed for you to reference to, to look at this change.
As an example, another is like, whenever I start, like I'll often do cleanup before I, like, if it finishes a task I had for it, I'll, I'll do a cleanup slash command and that will go through and tell to update all the docs, uh, and, and plan this next phase.
Because you can pass arguments in with your slash command. You can say something after it and it'll pass it in as arguments into the, the command that gets run, so then it knows what it's gonna do next. Since update the docs and then I clear it.
And then once it's cleared, I have a command that's like fresh and the Fresh Command tells it to read all the documentation and everything in the project index to read the whole thing. And then it knows not just what I wanted to do next, but all the docs and not everything in the project. Obviously, it'll be way too much for the context, but it has this ified version, uh, like a simple, uh, version of the entire project.
What's dependent, what, what's where. So Claude does a really great job of using the search tool and going and finding things right.
But the, the bad thing is, you know, you could, sometimes it will miss stuff, right? If it's really big, or sometimes it, the, the worst case scenario when you're vibe coating is it creates something in one place that it should have refactored in another, right?
That's like what happens is when people try to avoid, this isn't working, the project's too big. If it's small, it's simple. Everything fits in context and it just works.
If it's big, you have this problem. So to avoid this problem, I have a like bulletproof ish solution called the project index that works and that works off a hook. Wow. So in practice, what's the setup like to get this going?
Like how do I go from, I just installed cloud code to now. This all starts for me with Cloud Code Docs locally, and that's a public repository. I can, um, well can share it. It's, it's Cloud code Docs.
Uh, it's on, it's my repository. It's my project. But the project index, I don't have share at the moment, but I could share it, uh, in its.
It's basically, it'll be like a one line install and then you can just run it one time and then it, once that project index file exists in the project, it also installs the hooks. And so it'll just see is there a project index in this, in this folder?
If so, then the hook will trigger and it will just create the index of the project for you. So, but if you don't have that, I, this is what I built for myself, I'm happy to share it. But if you don't have something like that, but you want something like that, you can just say, Hey Claude, I'll do forward slash docs if you have that installed.
What do I need to do? Uh, in order to improve, you know, you, you gave me this result.
I didn't like the result. How can I talk to you better in order to prevent you from giving me this result? And gimme one more like that.
Uh, reference your documentation to see if there's anything useful that I, I could use to help, you know, encourage you in the right direction.
And just conversations like that, like asking it this messed up. I want it to not be like that. What would you recommend?
I change ultra. Think about it. Make a plan and then reference the docs and it will tell you all of these really crazy, interesting solutions. Like I've found so many things that, that I guess aren't really, why aren't used a lot, like passing context from a subagent, calling a sub agent, not just a subagent that defined early, but like.
Femoral one that you get to find for the task at hand and then gets removed or, or getting the context out of a subagent and passing it into a Claude that you can resume or taking the content from an existing session and moving it to another session through a background. There's, there's just a ton of things that you just talk to the docs and ask it, can I do this? Well, and here's your project folder. Claude, can you look at that and see what else I can do in addition to your docs?
And it'll tell you. Claude code is awesome at being able to allow us to get real world work done.
And to do that you need to share your real data and systems, and that's done through MCP and that can be a little bit scary.
So that's why I've been using Tool Hive. Tool Hive makes it simple and secure to use. MCP. It includes a registry of trusted MCP servers.
It lets me containerize any other server with one single command. I can install it in a client in seconds.
And secret protection and network isolation are built in. You can try Tool Hive too. I highly recommend you. Check it out.
It's free and open source, and you can learn more at toolhive.dev. Now back to the conversation with Ray and Eric.
For hooks, just just for the, the mental model, is it something you should think of as an automated slash command, or how do you differentiate what should be, what functionality should be a hook versus what you can put as a slash command? This is what I do. Um, I am pretty rigorous about trying to maintain the context, like knowing what's in the context. And you can think of it like, um. And ever since the beginning, this is like clearly the models are trained, the reinforcement learning, they have a knowledge cutoff. They're put into use, they do inference at that point.
What they know is, is a combination of like the hyper parameters, like what, you know, what, what's their temperature, their topic, all these things that are like tell how much compute to use, right? But then they have a system prompt that's usually not controlled by you unless you use subagents.
And subagents are different than the regular, Hey, Claude, create a subagent. If you have a defined subagent, you can tell it.
The system prompt to you, so it has a higher priority. Um, but there are trade offs to all this stuff because it's like you're receiving an email and someone's telling you, Hey, what really matters is the second paragraph. But the second paragraph here is the analogy of the prompt.
So people are focusing on like, the prompt is what matters. It does matter, but everything that email matters to you, like it comes from your boss. That's like the system instructions. Hey, here's an email. I want you to really focus on this to the customer and or like the.
External domain, uh, you know, notice that you can't forward it along or whatever. That kind of thing.
Those are, um, information that maybe you didn't control. Maybe it's a system message or something like that, but that's all part of the, what you're trying to figure out as a user or in this case, like what the agent knows is everything. And some, some of it, you know, some of it you.
But of the part you can control. It's really important to make sure that there's not extra stuff in there to confuse the model.
So I try to remove other, like if you put a bunch of MCP servers and a bunch of, um, subagents, well, Claude, if you ask Claude, what do you know?
Right now, like you just start a new cloud product project and you, uh, new session, you say, what do you know it knows about?
Its project three, uh, directory. It knows a little bit of metadata about the project. It knows the Cloud MD from the root, uh, the project of the user route and from the project route. And it knows about its hooks, like a brief little list.
Sorry, I not hooks. It does not know about its hooks. It knows about its slash commands.
A brief little one line description or something, and it knows a whole lot about its subagent.
And I don't like that because I don't want every command that I send for it to read a whole bunch of information that it, it may not be relevant for the task at hand. And it has to decide between, should I call this subagent, should I not call this subagent? So I don't use them unless it's like something where it's, it's very consistent for this type of project that's gonna need that. And it's usually at the project level, but for hooks, it doesn't know anything.
It doesn't dilute the context at all. So hooks exist outside of the lifecycle of, so there's a, there's a software development lifecycle, like a, a runtime, and then it's like before the hook is, before the, you know, you submit a request and there's all these stages to process that request. And there's certain points where there's a check to see if there's a hook there, and there's a, a, if there is, and it runs the code. But Claude itself, the context you're talking to doesn't know about it, which means that it's not messing with the context in any. when you would go to a slash crane versus a hook like I understand about polluting the context, but let's just say for, for general functionality like your, your docs slash docs, why, why would that benefit from a slash command versus a hook?
I created this before hooks were out. Um, it's possible that if I did docs, um, as a slash command, the thing is Claude really is instructed, I guess, in the system prompt, um, Claude code to look in their website for their documentation. So I tried to put the instructions in claude.md and it would just ignore them.
It would like inconsistently follow them anyway. But when I did it as a, uh, a slash command, then the slash command could tell. It could instruct Claude at the level directly what the user's doing.
It seemed to have a higher priority than the Claude MD being read in, or the project level, cloud project level is higher priority it seems, than the, the root level for user level for some reason. Um, but then your direct commands are higher level than all of them still.
And it might just be a recency thing, like where it falls in the whatever. But for whatever reason, when I did a, um.
A slash command, it consistently followed them. 'cause I could say, look in this re local repository for the docs.
Now if I do a hook, it has to be, it, it does a fetch, right? So it's, it's got a hook in there for the docs, for example.
And it will do the fetch. And if it's it, it'll do a pull if there's more information. But I think I tried that and I ran into some kind of issue because remember, that's running outside of your, uh, cloud contacts. You can inject stuff from the hook, I think, into the clo.
Well, you definitely can, but I just don't know if it's supported. There's lots of things you can do that I'm not sure how.
And, and also I just wanna pivot one really quick and say there's like, I'm always on the fence.
It's a gray area between, you know, you don't wanna do anything that could, uh, you know, be investigating.
Philanthropic stuff, right? But these models, it is kind of tricky because you ask, Hey, uh, look at these new tools and tell me what you're capable of doing. Or, Hey, I want to do this thing, but I'm not sure how to do it.
Um, and it will go off and figure out it's sandbox, it's environment. I like a lot of these tools.
I won't say which tool and which environment, but I asked it to, to just do a little research to tell me how I could do this thing.
And it created a mechanism that like basically broke out of, its, this thing got me, gave me all stuff that I, I, I thought, I don't think I'm supposed to notice this, and so I had to report it, but I'm just saying it's very easy to get further down the road than you want to get.
Subagent are a great example. I don't use them. Very much because, um, the default, if you say subagent has been around in Claude for a long time, you can just say, Hey, create a subagent. And it's a task, it's a tool and it's a task.
And it's called a general purpose. Subagent and philanthropic defines. Uh, what ITSs system prompt is, but it can go and do anything you want.
You can get 10 of 'em in parallel at one time, and they'll just do a, do a whole, do research or different use cases for different mechanisms to do it.
And Claude will manage it all. And if you, you know, stop, escape or whatever to interrupt, it'll handle the interrupt and all that stuff. It, you do your own version of it where you have your own subagent and it's not actually a subagent, it's really like a headless cloud code that you're calling, uh, then you have to handle the interrupts and all that kind of stuff.
So basically, subagent. I use them whenever, um, there's a task that's really repetitive, like the, I know exactly I want to do, I want to be dedicated and really good at doing this one specific thing, doing research on some tasks that I needed to pull out.
But I don't wanna create a lot of them because I don't want them to be confusing. The context of the, and a lot of times slash commands are my preferred use case.
And Ray, on one of your live streams, I saw you put together subagent. It was either for coding styles or for design styles.
When do you go to a, a sub agent? Yeah, for subagent, I've been kind of experimenting with this because.
My goal is to get some consistency and the only consistency I've received from subagents have been for research tasks in my code. And these are the tasks where I do wanna spend a lot of tokens to go through to make sure I don't repeat code. And so I think that's kind of what's been happening is like I started with my vibe idea, and then it's just kind of blowing up with more and more and more features. And when you start to add databases, authentication, and all these different patterns. It starts to get pretty interesting to see what the model prefers.
And so, uh, I generally spawn off like a subagent and I say, you know, to like, to the main task, like the goal right now is I want to implement authentication and here's some like documentation.
Can you just do a quick review on like where all this stuff and where I should be putting in, considering, you know, my client side is here and here's my, my my backend side. And so it's just gonna start digging through code files and I'm basically just ba giving an intern a task in some way to say, if.
Just dig through the code, gimme all the pieces that I should be aware about so that when I do my own code reviews, I can actually review and making sure that these lists are kind of all checked off. Um, because one thing I discovered in this entire process too is the fact that the model will say that it did one specific task, even though there's like a whole bunch of them, market is complete and move on.
And that's something that you have to be careful of as well. And so I like to give these subagent as like a second look.
In that whole pass. And sometimes I'll just give it the instructions and saying, here's my, here's my manifest of like areas where the authentication, um, was supposed to be implemented. Can you just do a quick pass? On each of these sections, and I want you to kind of think about how each subagent should take a look at it and just, I'm just kind of delegating it for the model. So at this point I have Opus and I have the Max Plan.
Opus is a really great orchestrator. It will kick off other subagents and those subagents still use Opus.
I found out, I thought it uses the next lower model, but I guess maybe 'cause I have the higher version of the plan, it's just uses another version of Opus.
And Opus is really, really, really good at grabbing lots of obscure information. You'll see it do lots of tool calls in that subagent.
So yeah, I basically treated the subagents right now as code reviewers and as like secondary reviewers through the different files.
And then I can kind of quickly suss out, it's like, hmm. This pattern looks like it's been repeated a whole bunch of times.
You know, it's just like my worst scenario. It's like I, I generated a bunch of code and it's just, you know, we could have just taken care of this in a, in a react hook of some sort. And that pattern should just repeat, be repeated everywhere else.
And, uh, those are the times I just kind of back out the change and then just start fresh again and say, let's go ahead and implement this, and this is kind of what a bad example looks like. And just literally copy and paste some of those examples and start a, a fresh new prompt and everything again.
I think that. Helps the model, steer the model, uh, to generate way better output and be thoughtful about its architecture.
And so I, I can basically go from, right now I spend a lot more time, I'd probably say I'd spend like 60 to 70% more time now in planning phases.
And, and, and code reviewing. So those, those are kind of like my splits right now. And then the other parts of the generation are just to sort of, kind of babysit and loop back. And that's kind of where I'm in this current phase right now. Uh, I wanted to do more automation.
I wanted to do more things, but as an engineer. I'm still spending a lot of time in this phase to verify all the stuff, and I'm actually very shocked at the output that it is not as good as what people are saying. I trust that it a little too much because this is what I'm discovering.
It's being over anxious about check, marking the boxes and moving on. So just wanna give people that type of heads up.
Yeah. trust but verify. I, I have, uh, I love Claude because, uh, you know, philanthropic does a lot of work to try to make sure, and there's a, there's a reason I'm an philanthropic fan. I don't know if we'll get to that at some point, but like, it does a lot of work on its personality and trying to, in instill principles and values and ethics and stuff into the models. But it's, um, it is very.
I don't trust Claude. I have to put in this instructions to, uh, you know, not lie to me.
Basically like, do not say that a thing is, is done if it's not done. That's dishonest.
I think that like gets into, its like, uh, really pays attention to that, right? And so when I talked about subagent earlier, I was really in the context of defined like predefined subagents. I do use a task subagent where you tell law to do a thing a lot, and that is largely for the purpose of, uh, doing something. Anything I can do that's outside the main context preserves the context of the main, uh. A cloud agent you're talking to. And so I don't need it to go and search a bunch of things that, um, it doesn't need to know about. Whenever those subagents can pass the information that it finds, it's relevant to the question back to the main agent, that it just has a subset of that knowledge is relevant. So that's the, the whole idea of maximizing the context is using those subs agents.
All the time in that way, but blind validation. So like before you do, this is what I found, uh, this is just for me to you, Ray. I don't know if else struggles with this, but when I say I don't trust Claude, I mean I explicitly do not trust it when it says it checked off a box that it's done.
I require in every case that it closes the loop with testing and that it doesn. It doesn't validate it has to have a subagent or some other agent be a blind validator. So in other words, when I'm starting a project, I'll first determine the plan and it has to include the testing plan and how specifically it's going to close the loop on testing.
Whether it's gonna use an MP MTP server, some other tool, it's gonna build something, it's online. Something like puppeteer, if it's a or, or if it's a Python script, like taking screenshots of the GUI and, and saving them. But it can't be the agent, the one I'm talking to, that's building cannot be the agent to, to verify that, that the checklist, that it, the, the main agent made it.
At first or whatever was designing the, the plan for testing. Um, that agent can't be the one that's, that's checking to see if it's done.
It has to be a separate agent explicitly with the task of you are a, a blind validator of this thing.
And then it looks at the screenshots and it looks at the checklist and it determines if it's done or not. And it passes, like it updates that file, for example.
And so that's the only way that I'll, because otherwise, like right now the issue is. It, the model golf and do a bunch of things.
And then we'll come back and it'll say it's done. And then we check it and then we're like, oh, it's not done. 'cause I, I can see this thing, it didn't work.
Right. So it needs to have some kind of test that it can run to verify that it's done, but it shouldn't be the one to do it. And so you have a subdivision that takes care of that piece for you. And that way when you come back, it's actually reliably done.
And that takes extra work up front to set up. But it saves you so much time down the, down the stretch of the project.
And then also. Right now we're, you know, we come back in a few minutes or whatever, but there is no reason that tools can't be constructed at this moment with the technology as it currently is that will reliably let you give it a task and you come back in an hour and the whole thing has been validated.
Because the, the reason that I love Claude is that from the very, from early on, you've been able to make composability a, a foundational feature.
You give it a task and it can spin up other versions of itself that accomplish these other sides. So it can be dynamic like on the fly in the moment, based on the task at hand.
Create a set of subagents that are responsible for this and this other thing, and they go off and do it and then come back.
And so, and these can be nested. They don't just have to be in parallel, like Subagent can't. Regular subagent can't form other subagents nested, but you there is a way around because of Claude code, because of not just the SDK, which has its different set of, uh, features and trade offs, but Claude in headless mode can be called by Claude and it can in turn claw Claude headless mode and subagents and things like that.
So you can imagine like this big tree that's a tree and context sharing and all that stuff in between withum ability.
So it's, it's possible now and people are building it. I've been working on some versions of it myself. Yeah. I want to also pull back a little bit too, and like I would not get discouraged if you're kind of maybe listening to this conversation and you're like, oh my God, this is way too far advanced.
I. I dropped outta school, right? I got into Apple just by pure grit and then worked my way up.
And I learned a lot of these software engineering practices, literally on the job, right? And then got to solve like bigger problems and bigger problems.
So like multimillion to billion dollar problems, right? And I think there's a core of truth that if you're just getting started out with this, don't be discouraged. You can go a long way simply by just acting as a user and literally using your product. And then trying to solve that one feedback loop, and that's gonna just, you, you, that's like the 80 20 of everything I feel is that like, okay, how do I not do this again? Or how do I set up a system to help me verify it? If you just start at that level of curiosity, it's gonna take you a very long way because now you say, oh, maybe I could set up a subagent for this to help me do this. Or How can I prompt the model to help me do this so that next time I, I implement a feature. As I'm testing it by hand, I don't hit that same problem or something.
And then you'll start to kind of discover some of these workflows. So I wanna encourage people that you can see how, why so many developers are extremely excited right now is because the capability is like literally just almost infinite.
It does get infinite and, and it goes not only infinite in like one direction. It's in in many directions.
And that's how. Big of this, you know, glacier ice pool or like this giant piece of mass that is this AI system and how different people with all these different perspectives are, are talking about it.
So I wouldn't be discouraged if you're listening to this and you maybe not know what half of the things are, um, but some of the basic tooling to just get started, it's like find one thing that you're doing. If it's being repeated a lot. Can you think about a system that you can repeat over and over again?
To make it more reliable for the next time to help you solve that problem. And that will like kind of help you in your learning journey.
And I think the other important thing for me has been just asking Claude code itself to help me with that.
Like what does that look like? Because I don't know, you know, like I didn't know how to implement a good subagent or something like that. And they've luckily have implemented this type of thing. If you do slash agents, you can actually just have a natural conversation about what you want to do. Then it generates the system prompt for you. So you don't have to be a prompt engineer, you don't have to give it good and bad examples. You don't have to do all these different techniques. You know, those are like more advanced, I'd say. But that already gets you a pretty long ways just in using that type of system, uh, in there. And so I, I mean, I only advice is like always keep it simple and sometimes the simpler the better. So if you, you feel like you're kind of going off a deep end, it's okay to kind of clear everything out and start fresh again.
It can help you learn things. The models are constantly evolving. We're constantly evolving in our knowledge.
We're learning more, we're demanding more from our ai now that we understand what this real thing is, right?
So it just kind of, uh, my, my overall lesson is like, don't be discouraged. This is a really great time to be alive and, you know, ask us more questions and reach out wherever you wanna reach out at and so forth. Uh, get ahold of us. Yeah.
Absolutely agree. I've been seeing more and more people trending around, oh, use TDD to, to operate your Claude. And you don't need to know what test driven development is.
You just have to think of the principle, write a test, make sure it passes the test. Eric, would you say there's any other.
Guidelines, principles for people to just do this exploration, this play, this discoverability of the capabilities or any advice on just, you know, getting started and getting that comfort level of just, you know, experimenting with the tool I mean, I really feel like, um, obviously the adage of just use it, so, I feel like everything that doesn't go right is an opportunity to figure out how can I talk to it better?
Um, and this has been sort of like from the beginning, if you can just think about like what we're leveraging here. By being a person who's using a tool like this, um, you're really setting yourself.
F apart by, because this is the flywheel, right? If you're, if you're like, every moment you spend getting through, like grinding on what is the difficult thing that most people stumble with, what, what, what failed here?
And then learning how do I overcome that by using the tool to help learn about it. And then continuing just that process.
Then you develop your own system and it changes from project to project and project size and style to project size and style.
And then you kinda get a feeling for it. Like, what I'm doing is not anything special or different than what anyone is able to do is just, um, I've spent time. With the tool and asking it a lot of questions whenever something didn't work right. And so based on that, I found, okay, well I, I don't wanna, like, there's friction between, I've got a chat conversation. It's like this, this, um. You know, you get a long email thread from someone.
We'll go back to that analogy. I'm just top of my head. I don't know if it works. But then like they, there's 16 messages, but it's fresh every time.
You've never seen it before and you've gotta read through all this stuff to figure out where you were.
Um, if you have to clear that email thread and start afresh, well, there's stuff that you would want.
To carry over. Not all of it, but some small subset of it that's really relevant. And that's the process of finding a, a system to preserve the right context as you clear your previous session right? It will handle all the compact for you. All of these tools, all of these.
Like cursor and windsurf and different models and wrappers for these models. Um, try to solve that problem as easily as possible.
But there's always still some friction there, like which context matters for the next session and how do you make sure you get it over?
And I think the persistence of the file mechanism, just riding, having it like. Update the documentation, write the next steps to the file that will be read in, and then as soon as you click clear, you, you run the thing that caused it to read that documentation, to know where to go.
I think that system is, uh, you know, is very, very valuable I think this is a good primer for people to get started with Claude code and ask more questions and get a little bit more perspective on some of our use cases. I feel like this is just scratching the surface.
For understanding where things can go and I, I would encourage people like, like Eric is saying, just be curious.
Start playing, uh, maybe chunk down to like, maybe one problem that you're trying to solve that you're, you know, you see that little bar there, you know, start asking the questions, trying to see if you can actually kind of work you through a workflow. I feel like it, a lot of people have these different areas of, of concern that you know, that they can kind of go down these rabbit holes. And I think these tools can get very advanced very quickly.
And just, if anything, if I have one piece of advice to give anyone, it's like your context window is so important and protect it with all mighty power, like it's your firstborn child and you don't want to let it go ever. So, um, that type of thinking will kind of help you get the most out of this like intelligence that we have today. And. I guess you're, um, if people want, find out some more, I, I do AI live streaming, so my name Mark, my handles, uh, Ray Fernando, 1, 3, 3 7, that's my YouTube channel.
You could also find me on XI post a lot on there as well. And so I, I, I do AI live streaming several times a week and you can find me live streaming. I'm also gonna have some recorded content kind of going over this concepts and stuff. So yeah, appreciate you having me on the show, Mike.
Yeah. I just wanna say it's a great pleasure to get to hang out with you guys. It's really fun to find people who are also interested in the same things around the world, uh, or like-minded and interested in sort of driving forward in the possibility of like what things we can unlock for ourselves and our families and our. Our future basically to make simple, to give us more time for the things we wanna do.
And it is just a really fun sandbox and tool to be able to do so much.
You don't maybe yet know the value when you're gonna find something in the future like, oh, I wish I could do that.
But you've learned the skills to do that and to talk to AI in the right way with the right tools to be able to do that. It.
It's very fun and I just wanna encourage all of you guys, uh, especially like Ray, it's really great to finally meet you.
I've been a fan for a while. I really appreciate it. Mike, I was following you at open interpreter. I think like it's, uh, it's been a journey and I don't know what it's gonna go, but I know that the future is bright and I, I really appreciate the ability, uh, to be able to get on and, and talk and share some of this stuff.
I've been kind of hidden, uh, doing my thing, talking to people one-on-one in the background, uh, involved in some interesting stuff.
So, but anyone who's doing this, I think, uh, you're spending your time very wisely because this is a good use and the, the payoff is gonna be really good long term.
Thank you for tuning into this conversation on Claude Code with Ray Fernando and Eric Buess. I had a great time talking to 'em.
Both guys are, are two phenomenal people who I really enjoy hanging out with. We hope that you gained value outta this because I know I did.
I, I learned stuff from these two just in this conversation, but there's so much more we want to cover. We were limited by time and we could have kept going for hours, so there will be a part two. Maybe here. Maybe it'll be on one of Ray's live streams or maybe on Eric's Twitter, so keep tuned for that.
Please follow both of them. They're phenomenal people. And I just wanna give a quick shout out to Tool Hive, the secure MCP servers that really help make you more comfortable sharing your personal information. And Claude code uses MCP servers, so you can definitely tie in there if you have any other questions, you know where to find us. Thank you for joining. We'll see you next week.